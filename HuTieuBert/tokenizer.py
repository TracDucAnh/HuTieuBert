import torch
import re
import unicodedata
import py_vncorenlp
from transformers import AutoTokenizer

class MorphemeAwareTokenizer(AutoTokenizer):
    def __init__(self, pretrained_model_name="vinai/phobert-base", vncorenlp_dir='/content/vncorenlp', **kwargs):
        # Kh·ªüi t·∫°o tokenizer HF g·ªëc
        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, **kwargs)

        # Kh·ªüi t·∫°o VnCoreNLP cho word segmentation
        self.rdrsegmenter = py_vncorenlp.VnCoreNLP(
            annotators=["wseg"],
            save_dir=vncorenlp_dir
        )

    def __len__(self):
        # Tr·∫£ v·ªÅ vocab size
        return self.vocab_size

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, vncorenlp_dir='/content/vncorenlp', **kwargs):
        """
        Load tokenizer t·ª´ Hugging Face v√† gi·ªØ logic custom
        """
        return cls(pretrained_model_name_or_path, vncorenlp_dir=vncorenlp_dir, **kwargs)
    

    @property
    def mask_token(self):
        return self.tokenizer.mask_token

    @property
    def pad_token(self):
        return self.tokenizer.pad_token

    @property
    def cls_token(self):
        return self.tokenizer.cls_token

    @property
    def sep_token(self):
        return self.tokenizer.sep_token

    @property
    def unk_token(self):
        return self.tokenizer.unk_token

    # =============================
    # ‚úÖ B·ªï sung ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi DataCollator
    # =============================

    @property
    def mask_token_id(self):
        return self.tokenizer.mask_token_id

    @property
    def pad_token_id(self):
        return self.tokenizer.pad_token_id

    @property
    def cls_token_id(self):
        return self.tokenizer.cls_token_id

    @property
    def sep_token_id(self):
        return self.tokenizer.sep_token_id

    @property
    def vocab_size(self):
        return self.tokenizer.vocab_size

    def pad(self, encoded_inputs, padding=True, max_length=None, return_tensors=None, **kwargs):
        """
        Cho ph√©p DataCollatorForLanguageModeling s·ª≠ d·ª•ng pad() nh∆∞ tokenizer Hugging Face.
        """
        return self.tokenizer.pad(
            encoded_inputs,
            padding=padding,
            max_length=max_length,
            return_tensors=return_tensors,
            **kwargs
        )

    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):
        """
        Tr·∫£ v·ªÅ mask cho special tokens (1 = special token, 0 = normal token).
        C·∫ßn thi·∫øt cho DataCollatorForLanguageModeling.
        """
        return self.tokenizer.get_special_tokens_mask(
            token_ids_0=token_ids_0,
            token_ids_1=token_ids_1,
            already_has_special_tokens=already_has_special_tokens
        )

    def convert_tokens_to_ids(self, tokens):
        """
        Chuy·ªÉn tokens th√†nh IDs. C·∫ßn cho m·ªôt s·ªë collator.
        """
        return self.tokenizer.convert_tokens_to_ids(tokens)

    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):
        """
        Chuy·ªÉn IDs th√†nh tokens.
        """
        return self.tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=skip_special_tokens)

    def to_bmes(self, text):
        """
        T·∫°o danh s√°ch (syllable, BMES-tag) t·ª´ text ho·∫∑c list[text].
        N·∫øu l√† list -> tr·∫£ v·ªÅ list[list[(syllable, tag)]]
        N·∫øu l√† str  -> tr·∫£ v·ªÅ list[(syllable, tag)]
        """
        if isinstance(text, list):
            return [self.to_bmes(t) for t in text]

        if not isinstance(text, str):
            text = str(text)

        segmented = self.rdrsegmenter.word_segment(text)
        
        # Tr∆∞·ªùng h·ª£p output l√† list nhi·ªÅu c√¢u ‚Üí g·ªôp l·∫°i theo t·ª´ng c√¢u ri√™ng
        if isinstance(segmented, list):
            sentences = segmented
        else:
            sentences = [segmented]

        bmes_list = []

        for sent in sentences:
            words = sent.split()
            for word in words:
                sylls = word.split("_")
                n = len(sylls)
                if n == 1:
                    bmes_list.append((sylls[0], 'S'))
                else:
                    bmes_list.append((sylls[0], 'B'))
                    for mid in sylls[1:-1]:
                        bmes_list.append((mid, 'M'))
                    bmes_list.append((sylls[-1], 'E'))
        
        return bmes_list


    def normalize_text(self, text):
        text = text.replace("@@", "").replace("‚ñÅ", "").strip()
        text = unicodedata.normalize('NFD', text)
        text = ''.join([c for c in text if not unicodedata.combining(c)])
        text = re.sub(r'[^\w\s]', '', text)
        return text.lower()

    def is_punctuation(self, text):
        normalized = re.sub(r'[^\w\s]', '', text).strip()
        return normalized == ""

    def align_bmes_to_subwords(self, bmes_list, subwords_list):
        """
        Align BMES tags v·ªõi subwords, x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p:
        - D·∫•u c√¢u d√≠nh v·ªõi ch·ªØ (vd: 'c.', '3.')
        - K√Ω t·ª± ƒë·∫∑c bi·ªát, <unk> tokens
        - Subword splitting ph·ª©c t·∫°p
        
        üîß FIX: X·ª≠ l√Ω <unk> token b·∫±ng c√°ch skip n√≥ v√† ti·∫øp t·ª•c alignment
        """
        aligned_tags = []
        syll_idx = 0
        buffer_raw = ""
        subword_positions = []
        
        i = 0
        while i < len(subwords_list):
            sub = subwords_list[i]
            
            # Special tokens - lu√¥n tag l√† 'S'
            if sub in ["<s>", "</s>", "<pad>", "<mask>"]:
                aligned_tags.append("S")
                i += 1
                continue
            
            # üîß X·ª¨ L√ù <unk> TOKEN
            if sub == "<unk>":
                # <unk> token l√† bi·ªÉu di·ªÖn c·ªßa 1 k√Ω t·ª± kh√¥ng ƒë∆∞·ª£c vocab nh·∫≠n di·ªán
                # G√°n tag 'S' cho n√≥ v√† b·ªè qua 1 syllable trong bmes_list n·∫øu c√≥
                aligned_tags.append("S")
                
                # N·∫øu c√≤n syllable, skip n√≥ v√¨ ƒë√£ ƒë∆∞·ª£c thay th·∫ø b·∫±ng <unk>
                if syll_idx < len(bmes_list):
                    syll_idx += 1
                
                # Reset buffer ƒë·ªÉ tr√°nh cascade errors
                buffer_raw = ""
                subword_positions = []
                
                i += 1
                continue
            
            # H·∫øt syllables - tag c√≤n l·∫°i l√† 'S'
            if syll_idx >= len(bmes_list):
                aligned_tags.append("S")
                i += 1
                continue
            
            # L·∫•y syllable hi·ªán t·∫°i
            syll, tag = bmes_list[syll_idx]
            clean_sub = sub.replace("‚ñÅ", "").replace("@@", "")
            
            # Normalize ƒë·ªÉ so s√°nh
            normalized_syll = self.normalize_text(syll)
            
            # Case 1: Syllable l√† d·∫•u c√¢u thu·∫ßn t√∫y
            if self.is_punctuation(syll):
                # Ki·ªÉm tra xem subword c√≥ ch·ª©a d·∫•u c√¢u n√†y kh√¥ng
                if clean_sub == syll or syll in clean_sub:
                    aligned_tags.append("S")
                    syll_idx += 1
                    i += 1
                    # Reset buffer n·∫øu ƒëang x·ª≠ l√Ω
                    buffer_raw = ""
                    subword_positions = []
                    continue
            
            # Case 2: Subword c√≥ d·∫•u c√¢u d√≠nh (vd: 'c.', 'i.')
            # T√°ch ph·∫ßn ch·ªØ v√† d·∫•u c√¢u
            word_part = ""
            punct_part = ""
            
            # Pattern ƒë·ªÉ t√°ch: ch·ªØ c√°i/s·ªë ·ªü ƒë·∫ßu, d·∫•u c√¢u ·ªü cu·ªëi
            match = re.match(r'^([a-zA-Z√Ä-·ªπ0-9]+)([^\w]+)$', clean_sub, re.UNICODE)
            if match:
                word_part = match.group(1)
                punct_part = match.group(2)
            else:
                word_part = clean_sub
                punct_part = ""
            
            # X·ª≠ l√Ω ph·∫ßn ch·ªØ
            if word_part:
                buffer_raw += word_part
                subword_positions.append(len(aligned_tags))
                aligned_tags.append(tag)  # Tag t·∫°m th·ªùi
                
                normalized_buffer = self.normalize_text(buffer_raw)
                
                # Ki·ªÉm tra buffer c√≥ kh·ªõp v·ªõi syllable ch∆∞a
                if normalized_buffer == normalized_syll:
                    # G√°n l·∫°i tags ƒë√∫ng cho t·∫•t c·∫£ subwords trong buffer
                    n = len(subword_positions)
                    if n > 1:
                        if tag == 'B':
                            aligned_tags[subword_positions[0]] = 'B'
                            for pos in subword_positions[1:]:
                                aligned_tags[pos] = 'M'
                        elif tag == 'E':
                            for pos in subword_positions[:-1]:
                                aligned_tags[pos] = 'M'
                            aligned_tags[subword_positions[-1]] = 'E'
                        elif tag == 'M':
                            for pos in subword_positions:
                                aligned_tags[pos] = 'M'
                        elif tag == 'S':
                            for pos in subword_positions:
                                aligned_tags[pos] = 'S'
                    else:
                        aligned_tags[subword_positions[0]] = tag
                    
                    # Reset buffer v√† tƒÉng syllable index
                    buffer_raw = ""
                    subword_positions = []
                    syll_idx += 1
                    
                    # X·ª≠ l√Ω ph·∫ßn d·∫•u c√¢u n·∫øu c√≥
                    if punct_part:
                        # Ki·ªÉm tra syllable ti·∫øp theo c√≥ ph·∫£i d·∫•u c√¢u kh√¥ng
                        if syll_idx < len(bmes_list):
                            next_syll, next_tag = bmes_list[syll_idx]
                            if self.is_punctuation(next_syll) or next_syll == punct_part:
                                # D·∫•u c√¢u n√†y thu·ªôc syllable ti·∫øp theo, kh√¥ng th√™m tag
                                syll_idx += 1
            
            i += 1
        
        return aligned_tags

    def __call__(self, text, **kwargs):
        # N·∫øu l√† list ‚Üí x·ª≠ l√Ω batch
        if isinstance(text, list):
            # 1. Tokenize c·∫£ batch b·∫±ng tokenizer g·ªëc
            encoded = self.tokenizer(
                text,
                add_special_tokens=True,
                padding=True,
                truncation=True,
                return_tensors=kwargs.get("return_tensors", None),
            )

            # 2. T·∫°o BMES tags cho t·ª´ng c√¢u
            BMES_MAP = {"B": 0, "M": 1, "E": 2, "S": 3}
            bmes_tags_list = []
            for i, t in enumerate(text):
                bmes_list = self.to_bmes(t)
                subwords = self.tokenizer.convert_ids_to_tokens(encoded["input_ids"][i].tolist())
                bmes_tags = self.align_bmes_to_subwords(bmes_list, subwords)
                
                # Chuy·ªÉn sang tensor n·∫øu c·∫ßn
                if kwargs.get("return_tensors") == "pt":
                    bmes_tags = torch.tensor([BMES_MAP[tag] for tag in bmes_tags])
                bmes_tags_list.append(bmes_tags)

            # Padding BMES tags gi·ªëng input_ids
            if kwargs.get("return_tensors") == "pt":
                max_len = encoded["input_ids"].shape[1]
                padded_bmes = []
                for tags in bmes_tags_list:
                    pad_len = max_len - tags.shape[0]
                    if pad_len > 0:
                        tags = torch.cat([tags, torch.full((pad_len,), BMES_MAP["S"])])
                    padded_bmes.append(tags)
                encoded["bmes_tags"] = torch.stack(padded_bmes)
            else:
                encoded["bmes_tags"] = bmes_tags_list

            return encoded

        # N·∫øu l√† string ƒë∆°n ‚Üí x·ª≠ l√Ω nh∆∞ c≈©
        bmes_list = self.to_bmes(text)
        encoded = self.tokenizer(text, add_special_tokens=True, **kwargs)

        input_ids = encoded["input_ids"]
        if isinstance(input_ids, torch.Tensor):
            input_ids = input_ids.squeeze(0).tolist()
        elif isinstance(input_ids[0], list):
            input_ids = input_ids[0]

        subwords = self.tokenizer.convert_ids_to_tokens(input_ids)
        bmes_tags = self.align_bmes_to_subwords(bmes_list, subwords)

        if kwargs.get("return_tensors") == "pt":
            BMES_MAP = {"B": 0, "M": 1, "E": 2, "S": 3}
            bmes_tags = torch.tensor([BMES_MAP[t] for t in bmes_tags]).unsqueeze(0)

        encoded['bmes_tags'] = bmes_tags
        return encoded
